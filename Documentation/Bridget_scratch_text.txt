You're in a strong position, having resolved the main issues and reduced the failing tests to just three. Here’s targeted advice for addressing the last mile:

⸻

1. Systematically Debug Each Remaining Test

For each failing test, follow this order:

A. Confirm Test Data Integrity
• Ensure that the baseline and current data are truly identical (especially for “no change” tests).
• For tests expecting warnings, re-calculate the percentage drift using your new logic to ensure the test data should trigger (or not trigger) a warning.

B. Check Configuration Alignment
• Match the test’s expected tolerance with the actual config used during validation.
• Sometimes, tests are written with comments or assertions that assume an old config value. Make sure both the config and the assertion match.

C. Examine Logic for Subtle Edge Cases
• For counts or performance, check for integer/floating-point rounding errors, e.g.:
Int(Double(2880) * 0.97) // May not always exactly hit the intended delta

• If you’re comparing floats, ensure you’re not hitting precision issues (consider using a tiny epsilon).

⸻

2. Use Logging or Temporary Prints

Add temporary logs in both your validation code and your test setup to output:
• The baseline and current values being compared
• The computed relative delta
• The tolerance being used
• The final pass/fail result for that check

This will make it much easier to spot where an unexpected comparison is happening.

⸻

3. Quick Checklist for Each Type of Remaining Test

• No Change Test Failing?
   • Double-check the test data creation methods for subtle clock, random, or order differences.
   • Confirm both current and baseline configs are identical and that the validator isn’t injecting any metadata.

• Change Detection Test Failing?
   • Confirm the test data’s difference genuinely exceeds the intended tolerance.
   • Make sure the test is not reusing a config with the wrong tolerance for the test’s intention.

• Edge Case Test Failing?
   • Check if zero or extremely small baseline values are causing division by zero or blowing up the relative change calculation.

⸻

4. If All Else Fails: Focus on the Simplest Test First

Start with the “no change” test—if this passes, you can be confident your basic logic is solid. Then tackle the warning-triggering tests.

⸻

5. Consider Adding Utility Assertions

To simplify debugging, add a helper for asserting “almost equal” for floating-point comparisons, e.g.:
func assertAlmostEqual(_ a: Double, _ b: Double, epsilon: Double = 1e-9) {
    XCTAssert(abs(a - b) < epsilon)
}

Summary

• Check test data for true equality/mismatch.
• Ensure configs match test expectations.
• Log/check calculations for each failing test.
• Start debugging with the simplest test.